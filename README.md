#  Projet de Navigation Autonome par Q-Learning

Ce projet impl√©mente un agent robotis√© capable de naviguer dans un environnement contraint pour accomplir une mission de logistique : r√©cup√©rer un objet et le livrer √† un point pr√©cis.

## Pr√©sentation du Probl√®me

L'objectif est de piloter un robot sur une grille de **4 lignes par 5 colonnes**. L'agent doit :
1. Partir du point de d√©part **D** (case (4,1)).
2. Se d√©placer vers l'un des points de collecte **O** (cases (1,1) ou (4,4)).
3. R√©cup√©rer l'objet et le transporter jusqu'au point de destination **S**.



###  Contraintes de l'Environnement
* **Obstacles fixes** : Certaines cases sont inaccessibles (marqu√©es en noir sur le plan).
* **Murs internes** : Des parois bloquent le passage direct entre certaines colonnes, obligeant l'agent √† contourner.

## ‚öôÔ∏è Mod√©lisation de l'Intelligence Artificielle

### 1. Espace d'√âtats
Le nombre total d'√©tats est de **40**:
* **20 positions** possibles (4x5).
* **2 variables d'√©tat** : Avec objet ou Sans objet.
* *Calcul : 20 positions √ó 2 √©tats de charge = 40 combinaisons possibles.*

### 2. Syst√®me d'Actions
L'agent peut effectuer **6 actions** distinctes:
* D√©placements : `Haut`, `Bas`, `Gauche`, `Droite`.
* Interactions : `R√©cup√©rer` (charger l'objet) et `D√©poser` (d√©charger l'objet).

### 3. Politique de R√©compenses
Le comportement de l'agent est dict√© par les scores suivants :
* **-1** : P√©nalit√© pour chaque mouvement (incite √† trouver le chemin le plus court).
* **-10** : P√©nalit√© pour une tentative de r√©cup√©ration ou d√©p√¥t hors zone.
* **+20** : Bonus pour une livraison r√©ussie au point S.

## üß†= Algorithme Utilis√© : Q-Learning

Le projet utilise l'algorithme de **Q-Learning** pour remplir une table de d√©cision (Q-Table). La mise √† jour de la connaissance se fait via l'√©quation de Bellman :

<img width="720" height="122" alt="image" src="https://github.com/user-attachments/assets/a4e1211d-dda9-4307-a63a-554480a2739b" />

* **Alpha (0.1)** : Vitesse √† laquelle l'agent int√®gre les nouvelles informations.
* **Gamma (0.9)** : Importance accord√©e aux r√©compenses futures.
* **Epsilon (0.2)** : Probabilit√© de tester des actions al√©atoires pour d√©couvrir de nouveaux chemins.

##  R√©sultats du Test
Une fois l'apprentissage termin√©, l'agent trouve le chemin optimal en 11 √©tapes pour r√©cup√©rer l'objet en (1,1) et le livrer en (1,5).

<img width="582" height="311" alt="image" src="https://github.com/user-attachments/assets/24d032ad-909a-4fdc-9682-78088167d499" />

> **Note :** Comme illustr√© ci-dessus, le robot optimise son trajet en √©vitant les obstacles et en effectuant les actions de chargement/d√©chargement aux coordonn√©es pr√©cises d√©finies dans l'√©nonc√©.



##  Comment l'utiliser ?

1. Assurez-vous d'avoir **Python 3** et **Numpy** install√©s.
2. Placez le fichier `travailAFaire.py` dans votre dossier.
3. Lancez l'apprentissage et le test :
   ```bash
   python travailAFaire.py

